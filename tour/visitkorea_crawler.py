"""
í•œêµ­ê´€ê´‘ê³µì‚¬ êµ¬ì„êµ¬ì„ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬
- ë©”ì¸ í˜ì´ì§€ì—ì„œ ì¥ì†Œ ì¹´ë“œ ëª©ë¡ ìˆ˜ì§‘
- ê° ì¥ì†Œì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
"""

import re
import time
import json
import argparse
from typing import List, Dict, Optional
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import pandas as pd


class VisitKoreaCrawler:
    """í•œêµ­ê´€ê´‘ê³µì‚¬ êµ¬ì„êµ¬ì„ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless: bool = False):
        """
        Args:
            headless: ë¸Œë¼ìš°ì €ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í• ì§€ ì—¬ë¶€
        """
        self.base_url = "https://korean.visitkorea.or.kr"
        self.driver = self._init_driver(headless)
        
        
    def _init_driver(self, headless: bool = False):
        options = webdriver.ChromeOptions()
        
        # 1. í˜ì´ì§€ ë¡œë“œ ì „ëµ ì„¤ì • (ì•ˆì •ì„±ì„ ìœ„í•´ eager ê¶Œì¥)
        options.page_load_strategy = 'normal' # í˜¹ì€ 'eager'
        
        if headless:
            options.add_argument('--headless')
        
        # ê¸°ë³¸ ì˜µì…˜ ì„¤ì •
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu') # ì¶”ê°€: ê·¸ë˜í”½ ê°€ì† ë” (ì¶©ëŒ ë°©ì§€)
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # ìœ„ì¹˜ ì •ë³´/ì•Œë¦¼ ì°¨ë‹¨
        prefs = {"profile.default_content_setting_values": {"geolocation": 2, "notifications": 2}}
        options.add_experimental_option("prefs", prefs)
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # [ìˆ˜ì •] ë“œë¼ì´ë²„ëŠ” ì—¬ê¸°ì„œ í•œ ë²ˆë§Œ ìƒì„±í•©ë‹ˆë‹¤.
        driver = webdriver.Chrome(options=options)
        driver.implicitly_wait(10)
        
        # Wait ê°ì²´ëŠ” ë“œë¼ì´ë²„ê°€ ìƒì„±ëœ í›„ì— ì—°ê²°í•©ë‹ˆë‹¤.
        self.wait = WebDriverWait(driver, 15)
        
        return driver
    
    def extract_location_ids(self, url: str = None, max_clicks: int = 1000) -> List[str]:
        if url is None:
            url = f"{self.base_url}/main/area_list.do?type=Place"
            
        print(f"ğŸ“ í˜ì´ì§€ ì ‘ì†: {url}")
        self.driver.get(url)
        time.sleep(5) 
        
        self._close_popups()
        
        # [ì¶”ê°€] ì§€ì—­ ë²„íŠ¼ë“¤ì´ ìˆëŠ” Swiper ì˜ì—­ì´ ë Œë”ë§ë˜ë„ë¡ ì‚´ì§ ìŠ¤í¬ë¡¤
        self.driver.execute_script("window.scrollTo(0, 150);")
        time.sleep(2)

        try:
            print("ğŸ” ì§€ì—­ í•„í„° ì„¤ì • ì‹œì‘...")

            # â‘  'ì„œìš¸' ë²„íŠ¼: í…ìŠ¤íŠ¸ ê¸°ë°˜ XPATHê°€ Swiper êµ¬ì¡°ì—ì„œ ê°€ì¥ ê°•ë ¥í•©ë‹ˆë‹¤.
            seoul_btn = self.wait.until(EC.presence_of_element_located((By.XPATH, "//a[.//span[text()='ì„œìš¸']]")))
            
            # ë²„íŠ¼ ìœ„ì¹˜ë¡œ ìŠ¤í¬ë¡¤ í›„ í´ë¦­
            self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", seoul_btn)
            time.sleep(1)
            self.driver.execute_script("arguments[0].click();", seoul_btn)
            
            print("   - íŒì—… ë¡œë”© ëŒ€ê¸° ì¤‘ (5ì´ˆ)...")
            time.sleep(5) 

            # â‘¢ ì²´í¬ë°•ìŠ¤ ìƒíƒœ í™•ì¸ (input íƒœê·¸) ë° í´ë¦­ (label íƒœê·¸)
            # mapAll ì•„ì´ë””ë¥¼ ê°€ì§„ ì‹¤ì œ input ìš”ì†Œê°€ ì²´í¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
            checkpoint = self.driver.find_element(By.ID, "mapAll")
            all_chk_label = self.driver.find_element(By.CSS_SELECTOR, "label[for='mapAll']")

            if not checkpoint.is_selected():
                print("   - í˜„ì¬ ë¯¸ì„ íƒ ìƒíƒœì´ë¯€ë¡œ 'ì „ì²´ì„ íƒ' í´ë¦­")
                all_chk_label.click() 
            else:
                print("   - ì´ë¯¸ 'ì „ì²´ì„ íƒ' ìƒíƒœì´ë¯€ë¡œ í´ë¦­ ìŠ¤í‚µ")

            self.driver.execute_script("""
                    var element = document.getElementById('mapAll');
                    var event = new Event('change', { 'bubbles': true });
                    element.dispatchEvent(event);
                """)
            print("   - ë³€ê²½ ì´ë²¤íŠ¸(Change Event) ê°•ì œ ì „ì†¡ ì™„ë£Œ")
            
            time.sleep(2) 

            # â‘¢ 'ì„ íƒ' ë²„íŠ¼ í´ë¦­ ì „, ê²½ê³ ì°½ì´ ì´ë¯¸ ë– ìˆë‹¤ë©´ ë‹«ê¸°
            try:
                alert = self.driver.switch_to.alert
                alert.accept()
                print("   - ë¯¸ë¦¬ ëœ¬ ê²½ê³ ì°½ì„ ë‹«ì•˜ìŠµë‹ˆë‹¤.")
            except:
                pass
            
            time.sleep(2) # í´ë¦­ ìƒíƒœ ë°˜ì˜ ëŒ€ê¸°            
            
              # 'ì„ íƒ' ë²„íŠ¼ ì°¾ê¸°
            apply_btn = self.wait.until(EC.presence_of_element_located((By.XPATH, "//*[text()='ì„ íƒ']")))
            
            # [ì¤‘ìš”] íŒì—… ë’¤ í˜ì´ì§€ê°€ ì›€ì§ì´ì§€ ì•Šë„ë¡, ìŠ¤í¬ë¡¤ ì—†ì´ ë°”ë¡œ JS í´ë¦­
            self.driver.execute_script("arguments[0].click();", apply_btn)
            print("âœ… 'ì„ íƒ' ë²„íŠ¼ í´ë¦­ ì™„ë£Œ")
            
            # íŒì—…ì´ ë‹«íˆê³  ë°ì´í„°ê°€ ìƒˆë¡œê³ ì¹¨ë  ë•Œê¹Œì§€ ì¶©ë¶„íˆ ëŒ€ê¸°
            time.sleep(5) 

        except Exception as e:
            print(f"âŒ íŒì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# 1. í˜ì´ì§€ í•˜ë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ ë”ë³´ê¸° ë²„íŠ¼ì´ ë¡œë“œë˜ê²Œ í•¨
        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

        # # í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ì»¨í…ì¸  ë¡œë“œ
        # self.driver.execute_script("window.scrollTo(0, 500);")
        # time.sleep(1)
        
        # ë”ë³´ê¸° ë²„íŠ¼ì„ ê³„ì† í´ë¦­í•˜ì—¬ ëª¨ë“  ì¹´ë“œ ë¡œë“œ
        click_count = 0
        while click_count < max_clicks:
            try:
                # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
                more_button = None
                
                # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                selectors = [
                    "//button[contains(text(), 'ë”ë³´ê¸°')]",
                    "//button[contains(@class, 'btn_more')]",
                    "//a[contains(text(), 'ë”ë³´ê¸°')]",
                    "//a[contains(@class, 'more')]"
                ]
                
                for selector in selectors:
                    buttons = self.driver.find_elements(By.XPATH, selector)
                    for btn in buttons:
                        if btn.is_displayed() and btn.is_enabled():
                            more_button = btn
                            break
                    if more_button:
                        break
                
                if not more_button:
                    print(f"  â„¹ï¸  ë”ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì´ {click_count}íšŒ í´ë¦­)")
                    break
                
                # JavaScriptë¡œ ë²„íŠ¼ í´ë¦­ (element click intercepted ë°©ì§€)
                try:
                    self.driver.execute_script("arguments[0].click();", more_button)
                    click_count += 1
                    print(f"  ğŸ”„ ë”ë³´ê¸° í´ë¦­ {click_count}íšŒ")
                    time.sleep(2)  # ë¡œë”© ëŒ€ê¸°
                except Exception as e:
                    print(f"  âš ï¸  í´ë¦­ ì‹¤íŒ¨, ì¬ì‹œë„: {str(e)[:30]}")
                    # ìŠ¤í¬ë¡¤ í›„ ì¬ì‹œë„
                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", more_button)
                    time.sleep(1)
                    self.driver.execute_script("arguments[0].click();", more_button)
                    click_count += 1
                    print(f"  ğŸ”„ ë”ë³´ê¸° í´ë¦­ {click_count}íšŒ (ì¬ì‹œë„ ì„±ê³µ)")
                    time.sleep(2)
                
            except Exception as e:
                print(f"  â„¹ï¸  ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì¢…ë£Œ: {str(e)[:50]}")
                break
        
        # í˜ì´ì§€ ì†ŒìŠ¤ íŒŒì‹±
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')
        
        # ì¥ì†Œ ì¹´ë“œì—ì„œ ID ì¶”ì¶œ (goDetailPage íŒ¨í„´)
        location_ids = []
        links = soup.find_all('a', href=re.compile(r'goDetailPage\('))
        
        for link in links:
            href = link.get('href', '')
            # goDetailPage('ì•„ì´ë””ê°’') íŒ¨í„´ì—ì„œ ID ì¶”ì¶œ
            match = re.search(r"goDetailPage\('([^']+)'\)", href)
            if match:
                location_id = match.group(1)
                if location_id not in location_ids:
                    location_ids.append(location_id)
                    
                    # ì¥ì†Œëª…ë„ í•¨ê»˜ ì¶œë ¥
                    name = link.get_text(strip=True) or "ì´ë¦„ ì—†ìŒ"
                    print(f"  âœ“ {name} (ID: {location_id})")
        
        print(f"\nâœ… ì´ {len(location_ids)}ê°œ ì¥ì†Œ ë°œê²¬ (ë”ë³´ê¸° {click_count}íšŒ í´ë¦­)\n")
        return location_ids
    
    def _close_popups(self):
        """
        íŒì—… ìë™ ì²˜ë¦¬ (ìœ„ì¹˜ ì •ë³´ ë™ì˜ ë“±)
        """
        try:
            # 1. ìœ„ì¹˜ ì •ë³´ íŒì—… ì²˜ë¦¬ (ë™ì˜ ë²„íŠ¼ í´ë¦­)
            try:
                # íŒì—…ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
                location_popup = WebDriverWait(self.driver, 3).until(
                    EC.presence_of_element_located((By.ID, "locationServicePop"))
                )
                
                # 'ë™ì˜' ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
                agree_button = self.driver.find_element(
                    By.XPATH, 
                    "//div[@id='locationServicePop']//a[text()='ë™ì˜']"
                )
                
                if agree_button.is_displayed():
                    # JavaScriptë¡œ í´ë¦­ (ë” ì•ˆì •ì )
                    self.driver.execute_script("arguments[0].click();", agree_button)
                    print("  âœ“ ìœ„ì¹˜ ì •ë³´ ë™ì˜ ì™„ë£Œ")
                    time.sleep(1)
                    return  # ë™ì˜ ë²„íŠ¼ í´ë¦­ ì„±ê³µ ì‹œ ì¢…ë£Œ
            except:
                # ìœ„ì¹˜ ì •ë³´ íŒì—…ì´ ì—†ìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ
                pass
            
            # 2. ê¸°íƒ€ íŒì—… ë‹«ê¸° (ë‹«ê¸° ë²„íŠ¼)
            close_selectors = [
                "//button[contains(text(), 'ë‹«ê¸°')]",
                "//button[contains(text(), 'ì·¨ì†Œ')]",
                "//button[contains(@class, 'close')]",
                "//button[contains(@class, 'btn_close')]",
                "//a[contains(@class, 'close')]",
                "//button[@aria-label='ë‹«ê¸°']",
                "//button[@aria-label='Close']",
            ]
            
            for selector in close_selectors:
                try:
                    buttons = self.driver.find_elements(By.XPATH, selector)
                    for btn in buttons:
                        if btn.is_displayed():
                            self.driver.execute_script("arguments[0].click();", btn)
                            print("  âœ“ íŒì—… ë‹«ê¸° ì™„ë£Œ")
                            time.sleep(0.5)
                            return  # í•˜ë‚˜ë§Œ ë‹«ê³  ì¢…ë£Œ
                except:
                    continue
                    
        except Exception as e:
            # íŒì—…ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
            pass
    
    def crawl_detail_info(self, location_id: str) -> Dict:
        """
        íŠ¹ì • ì¥ì†Œì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        
        Args:
            location_id: ì¥ì†Œ ID (cotid)
            
        Returns:
            ìƒì„¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        detail_url = f"{self.base_url}/detail/ms_detail.do?cotid={location_id}"
        print(f"ğŸ“„ ìƒì„¸ í˜ì´ì§€ ì ‘ì†: {location_id}")
        
        try:
            self.driver.get(detail_url)
            time.sleep(2)
            
            # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ì¥ì†Œëª…
            title_tag = soup.find('h2') or soup.find('h1', class_=re.compile(r'tit'))
            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
            
            # 'ìƒì„¸ì •ë³´' íƒ­ í´ë¦­ (JavaScriptë¡œ ì§ì ‘ ì‹¤í–‰)
            try:
                self.driver.execute_script("tabChange('detailGo');")
                time.sleep(2)  # íƒ­ ì „í™˜ ëŒ€ê¸°
                print("  âœ“ ìƒì„¸ì •ë³´ íƒ­ í´ë¦­ ì™„ë£Œ")
            except Exception as e:
                print(f"  âš ï¸  ìƒì„¸ì •ë³´ íƒ­ í´ë¦­ ì‹¤íŒ¨: {str(e)}")
            
            # ìƒì„¸ì •ë³´ íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            detail_info = self._parse_detail_section(soup)
            
            # ì‚¬ì§„ URL ì¶”ì¶œ
            photo_urls = self._extract_photo_urls(soup)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'id': location_id,
                'name': title,
                'url': detail_url,
                'photo_urls': photo_urls,
                **detail_info
            }
            
            print(f"  âœ“ {title} - {len(detail_info)}ê°œ í•­ëª©, {len(photo_urls)}ê°œ ì‚¬ì§„ ìˆ˜ì§‘")
            return result
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                'id': location_id,
                'name': 'ì˜¤ë¥˜',
                'url': detail_url,
                'error': str(e)
            }
    
    def _parse_detail_section(self, soup: BeautifulSoup) -> Dict:
        """
        ìƒì„¸ì •ë³´ ì„¹ì…˜ íŒŒì‹±
        
        Args:
            soup: BeautifulSoup ê°ì²´
            
        Returns:
            ìƒì„¸ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        detail_info = {}
        
        # <li> íƒœê·¸ë¡œ êµ¬ì„±ëœ ì •ë³´ í•­ëª© ì°¾ê¸°
        list_items = soup.find_all('li')
        
        for item in list_items:
            # í•­ëª©ëª… (Label)
            label_tag = item.find('strong')
            if not label_tag:
                continue
                
            label = label_tag.text.strip()
            
            # í•­ëª©ê°’ (Value) - span íƒœê·¸ì—ì„œ ì¶”ì¶œ
            value_tags = item.find_all('span', class_='pc') or item.find_all('span')
            
            if value_tags:
                # ì—¬ëŸ¬ spanì´ ìˆì„ ê²½ìš° í•©ì¹˜ê¸°
                values = [tag.text.strip() for tag in value_tags if tag.text.strip()]
                value = ' / '.join(values) if values else ''
            else:
                # spanì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ label ì œê±°
                value = item.text.replace(label, '').strip()
            
            if value:
                detail_info[label] = value
        
        return detail_info
    
    def _extract_photo_urls(self, soup: BeautifulSoup) -> List[str]:
        """
        ì‚¬ì§„ URL ì¶”ì¶œ
        
        Args:
            soup: BeautifulSoup ê°ì²´
            
        Returns:
            ì‚¬ì§„ URL ë¦¬ìŠ¤íŠ¸
        """
        photo_urls = []
        
        # ì‚¬ì§„ë³´ê¸° ì˜ì—­ (id="galleryGo")ì—ì„œ swiper ìŠ¬ë¼ì´ë“œ ì°¾ê¸°
        gallery_section = soup.find('div', id='galleryGo')
        if not gallery_section or not hasattr(gallery_section, 'find_all'):
            return photo_urls
        
        # swiper-slide ë‚´ì˜ ëª¨ë“  img íƒœê·¸ ì°¾ê¸°
        slides = gallery_section.find_all('div', class_='swiper-slide')
        
        for slide in slides:
            img_tag = slide.find('img')
            if img_tag:
                # src ë˜ëŠ” data-src ì†ì„±ì—ì„œ URL ì¶”ì¶œ
                img_url = img_tag.get('src') or img_tag.get('data-src')
                if img_url and img_url.startswith('http'):
                    # URLì—ì„œ &amp; ë¥¼ & ë¡œ ë³€í™˜
                    img_url = img_url.replace('&amp;', '&')
                    if img_url not in photo_urls:
                        photo_urls.append(img_url)
        
        return photo_urls
    
    def crawl_multiple(self, location_ids: List[str], delay: float = 1.0) -> List[Dict]:
        """
        ì—¬ëŸ¬ ì¥ì†Œì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        
        Args:
            location_ids: ì¥ì†Œ ID ë¦¬ìŠ¤íŠ¸
            delay: ê° ìš”ì²­ ì‚¬ì´ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            
        Returns:
            ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        total = len(location_ids)
        
        for idx, location_id in enumerate(location_ids, 1):
            print(f"\n[{idx}/{total}] ", end='')
            result = self.crawl_detail_info(location_id)
            results.append(result)
            
            if idx < total:
                time.sleep(delay)
        
        return results
    
    def save_to_csv(self, data: List[Dict], filename: str):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   ì´ {len(data)}ê°œ í•­ëª©")
    
    def save_to_json(self, data: List[Dict], filename: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   ì´ {len(data)}ê°œ í•­ëª©")
    
    def save_location_ids(self, location_ids: List[str], filename: str):
        """ì¥ì†Œ ID ëª©ë¡ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(location_ids, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ID ëª©ë¡ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   ì´ {len(location_ids)}ê°œ ID")
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("\nğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(
        description='í•œêµ­ê´€ê´‘ê³µì‚¬ êµ¬ì„êµ¬ì„ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ì‚¬ìš© ì˜ˆì‹œ:
  # 10ê°œ ì¥ì†Œ í¬ë¡¤ë§
  python visitkorea_crawler.py --count 10
  
  # 50ê°œ ì¥ì†Œ í¬ë¡¤ë§, ë”ë³´ê¸° 30íšŒ í´ë¦­
  python visitkorea_crawler.py --count 50 --max-clicks 30
  
  # ë°±ê·¸ë¼ìš´ë“œ ëª¨ë“œë¡œ 100ê°œ í¬ë¡¤ë§
  python visitkorea_crawler.py --count 100 --headless
        '''
    )
    
    parser.add_argument(
        '--count', '-c',
        type=int,
        default=10,
        help='í¬ë¡¤ë§í•  ì¥ì†Œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)'
    )
    
    parser.add_argument(
        '--max-clicks', '-m',
        type=int,
        default=1000,
        help='ë”ë³´ê¸° ë²„íŠ¼ ìµœëŒ€ í´ë¦­ íšŸìˆ˜ (ê¸°ë³¸ê°’: 1000)'
    )
    
    parser.add_argument(
        '--headless',
        action='store_true',
        help='í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰ (ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€)'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='visitkorea_data',
        help='ì¶œë ¥ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸, ê¸°ë³¸ê°’: visitkorea_data)'
    )
    
    args = parser.parse_args()
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = VisitKoreaCrawler(headless=args.headless)
    
    try:
        # 1. ì¥ì†Œ ID ìˆ˜ì§‘ (ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­)
        print("=" * 60)
        print("1ë‹¨ê³„: ì¥ì†Œ ID ìˆ˜ì§‘")
        print("=" * 60)
        location_ids = crawler.extract_location_ids(max_clicks=args.max_clicks)
        
        # ID ëª©ë¡ ì €ì¥
        crawler.save_location_ids(location_ids, f'{args.output}_ids.json')
        
        # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ í¬ë¡¤ë§
        target_count = min(args.count, len(location_ids))
        target_ids = location_ids[:target_count]
        print(f"\nğŸ“Š ì´ {len(location_ids)}ê°œ ì¤‘ {target_count}ê°œ ì¥ì†Œë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.\n")
        
        # 2. ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        print("=" * 60)
        print("2ë‹¨ê³„: ìƒì„¸ ì •ë³´ í¬ë¡¤ë§")
        print("=" * 60)
        results = crawler.crawl_multiple(target_ids, delay=1.5)
        
        # 3. ê²°ê³¼ ì €ì¥
        print("\n" + "=" * 60)
        print("3ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
        print("=" * 60)
        crawler.save_to_json(results, f'{args.output}.json')
        crawler.save_to_csv(results, f'{args.output}.csv')
        
        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! {len(results)}ê°œ ì¥ì†Œ ì •ë³´ ì €ì¥ë¨")
        
        print("\n" + "=" * 60)
        print("ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        print("=" * 60)
        for item in results[:2]:
            print(f"\nğŸ“ {item.get('name', 'ì´ë¦„ ì—†ìŒ')}")
            for key, value in item.items():
                if key not in ['id', 'name', 'url']:
                    print(f"   {key}: {value}")
        
    finally:
        crawler.close()


if __name__ == "__main__":
    main()
