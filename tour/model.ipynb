{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4d7d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.7 environment at: C:\\test\\.venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m17 packages\u001b[0m \u001b[2min 1.02s\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m torchvision \u001b[2m(4.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m scipy \u001b[2m(34.6MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m torchvision\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m scipy\n",
      "\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 3.41s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m15 packages\u001b[0m \u001b[2min 44.05s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2026.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.10.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.25.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install torch torchvision pillow numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa106f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[결과] 두 장소의 유사도 점수: 0.4207\n",
      "결과: 서로 다른 느낌의 장소입니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 1. 모델 로드 (DINOv2 Small 버전 - 로컬 사양 부담 적음)\n",
    "# 더 높은 성능을 원하시면 'dinov2_vits14' 대신 'dinov2_vitb14'를 쓰세요.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
    "model.eval()\n",
    "\n",
    "# 2. 이미지 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def get_image_embedding(image_path):\n",
    "    \"\"\"이미지에서 특징 벡터를 추출하는 함수\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_t = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_t)\n",
    "    return embedding\n",
    "\n",
    "def calculate_similarity(path1, path2):\n",
    "    \"\"\"두 이미지 사이의 코사인 유사도를 계산\"\"\"\n",
    "    emb1 = get_image_embedding(path1)\n",
    "    emb2 = get_image_embedding(path2)\n",
    "    \n",
    "    # 코사인 유사도 계산 (1에 가까울수록 비슷함)\n",
    "    similarity = F.cosine_similarity(emb1, emb2)\n",
    "    return similarity.item()\n",
    "\n",
    "# 3. 실행 예시\n",
    "# 노트북 파일이 있는 디렉토리를 기준으로 경로 설정\n",
    "# 절대 경로로 이미지 파일 지정\n",
    "path_a = r\"c:\\test\\tour\\img_forigen\\002.jpg\"  # 애니메이션/외국 사진\n",
    "path_b = r\"c:\\test\\tour\\img_korea\\002.jpeg\"    # 국내 사진\n",
    "\n",
    "try:\n",
    "    score = calculate_similarity(path_a, path_b)\n",
    "    print(f\"\\n[결과] 두 장소의 유사도 점수: {score:.4f}\")\n",
    "    \n",
    "    if score > 0.8:\n",
    "        print(\"결과: 매우 비슷합니다! 한국의 숨은 명소일 확률이 높네요.\")\n",
    "    elif score > 0.6:\n",
    "        print(\"결과: 분위기가 제법 흡사합니다.\")\n",
    "    else:\n",
    "        print(\"결과: 서로 다른 느낌의 장소입니다.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"이미지 파일을 찾을 수 없습니다: {e}\")\n",
    "    print(f\"찾고 있던 경로:\")\n",
    "    print(f\"  - {path_a}\")\n",
    "    print(f\"  - {path_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9a71fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[검색 결과 - 기준: img_forigen/001.jpg]\n",
      "1등: 001.jpg (유사도: 0.4898)\n",
      "2등: 004.JPG (유사도: 0.0484)\n",
      "3등: 002.jpeg (유사도: 0.0234)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 1. 모델 로드 (최초 실행 시 다운로드 진행)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
    "model.eval()\n",
    "\n",
    "# 이미지 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def get_embedding(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_t = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_t)\n",
    "    return embedding\n",
    "\n",
    "# --- 경로 지정 섹션 ---\n",
    "# 1. 기준이 되는 외국/애니메이션 이미지 (파일명이 001.jpg라고 가정)\n",
    "query_img = \"img_forigen/001.jpg\" \n",
    "\n",
    "# 2. 비교할 한국 이미지들이 들어있는 폴더\n",
    "target_dir = \"img_korea\"\n",
    "# -----------------------\n",
    "\n",
    "if os.path.exists(query_img) and os.path.isdir(target_dir):\n",
    "    query_emb = get_embedding(query_img)\n",
    "    \n",
    "    results = []\n",
    "    valid_ext = ('.jpg', '.jpeg', '.png', '.webp')\n",
    "\n",
    "    for file_name in os.listdir(target_dir):\n",
    "        if file_name.lower().endswith(valid_ext):\n",
    "            target_path = os.path.join(target_dir, file_name)\n",
    "            target_emb = get_embedding(target_path)\n",
    "            \n",
    "            # 유사도 계산\n",
    "            score = F.cosine_similarity(query_emb, target_emb).item()\n",
    "            results.append((file_name, score))\n",
    "\n",
    "    # 점수 높은 순으로 정렬\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\n[검색 결과 - 기준: {query_img}]\")\n",
    "    for i, (name, s) in enumerate(results[:3]):\n",
    "        print(f\"{i+1}등: {name} (유사도: {s:.4f})\")\n",
    "else:\n",
    "    print(\"경로를 찾을 수 없습니다. 폴더명이나 파일명을 확인해주세요.\")\n",
    "    # 현재 내 코드가 어디를 보고 있는지 확인용\n",
    "    print(f\"현재 작업 디렉토리: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27cbde68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.7 environment at: C:\\test\\.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m4 packages\u001b[0m \u001b[2min 827ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install transformers torch torchvision pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1bc3424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 398/398 [00:00<00:00, 1151.98it/s, Materializing param=visual_projection.weight]                                \n",
      "\u001b[1mCLIPModel LOAD REPORT\u001b[0m from: openai/clip-vit-base-patch32\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
      "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP 모델로 'img_korea' 내 유사 장소 검색 중...\n",
      "\n",
      "[CLIP 검색 결과 - 기준: img_forigen/002.jpg]\n",
      "1등: 002.jpeg (유사도: 0.8299)\n",
      "2등: 009.jpg (유사도: 0.6160)\n",
      "3등: 008.jpg (유사도: 0.5671)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. CLIP 모델 및 프로세서 로드\n",
    "# 가장 많이 쓰이는 ViT-B/32 버전을 사용합니다.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(model_id).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "\n",
    "def get_clip_embedding(image_path):\n",
    "    \"\"\"CLIP을 이용해 이미지의 특징 벡터를 추출\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 여기서 출력을 받습니다.\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "        \n",
    "        # [수정 포인트] 만약 outputs가 객체 형태라면 데이터(Tensor)만 추출합니다.\n",
    "        # 일반적으로 CLIPModel은 직접 Tensor를 반환하지만, \n",
    "        # 특정 버전에서는 객체를 반환할 수 있어 아래와 같이 처리하는 것이 안전합니다.\n",
    "        if hasattr(outputs, \"pooler_output\"):\n",
    "            image_features = outputs.pooler_output\n",
    "        else:\n",
    "            image_features = outputs\n",
    "            \n",
    "        # 이제 정규화를 수행합니다.\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "    return image_features\n",
    "\n",
    "# --- 경로 설정 ---\n",
    "query_img_path = \"img_forigen/002.jpg\"\n",
    "target_dir = \"img_korea\"\n",
    "\n",
    "if os.path.exists(query_img_path) and os.path.isdir(target_dir):\n",
    "    # 1. 기준 이미지 벡터 추출\n",
    "    query_emb = get_clip_embedding(query_img_path)\n",
    "    \n",
    "    results = []\n",
    "    valid_ext = ('.jpg', '.jpeg', '.png', '.webp')\n",
    "\n",
    "    # 2. 한국 이미지 폴더 순회 및 비교\n",
    "    print(f\"CLIP 모델로 '{target_dir}' 내 유사 장소 검색 중...\")\n",
    "    \n",
    "    for file_name in os.listdir(target_dir):\n",
    "        if file_name.lower().endswith(valid_ext):\n",
    "            target_path = os.path.join(target_dir, file_name)\n",
    "            target_emb = get_clip_embedding(target_path)\n",
    "            \n",
    "            # 3. 코사인 유사도 계산\n",
    "            # CLIP은 이미 정규화를 했으므로 단순 내적(dot product)으로도 계산 가능하지만, \n",
    "            # 명확성을 위해 동일하게 F.cosine_similarity를 사용합니다.\n",
    "            score = F.cosine_similarity(query_emb, target_emb).item()\n",
    "            results.append((file_name, score))\n",
    "\n",
    "    # 결과 정렬\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\n[CLIP 검색 결과 - 기준: {query_img_path}]\")\n",
    "    for i, (name, s) in enumerate(results[:3]):\n",
    "        print(f\"{i+1}등: {name} (유사도: {s:.4f})\")\n",
    "else:\n",
    "    print(\"경로를 확인해 주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d157ee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.7 environment at: C:\\test\\.venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m30 packages\u001b[0m \u001b[2min 350ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m timm \u001b[2m(2.4MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m timm\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 258ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 166ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtimm\u001b[0m\u001b[2m==1.0.24\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c2a09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\test\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--timm--convnextv2_tiny.fcmae_ft_in22k_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# V2의 작은 모델인 'atto'나 'tiny' 버전을 추천합니다.\n",
    "model_convnext = timm.create_model('convnextv2_tiny.fcmae_ft_in22k_in1k', pretrained=True, num_classes=0).to(device)\n",
    "model_convnext.eval()\n",
    "\n",
    "def get_convnext_embedding(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    # timm 모델 전용 전처리 구성\n",
    "    data_config = timm.data.resolve_model_data_config(model_convnext)\n",
    "    transform = timm.data.create_transform(**data_config, is_training=False)\n",
    "    \n",
    "    img_t = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model_convnext(img_t) # num_classes=0이면 풀링된 특징 벡터가 나옵니다\n",
    "    return features / features.norm(p=2, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5e58a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Places365 검색 결과 - 기준: img_forigen/001.jpg]\n",
      "1등: 001.jpg (유사도: 0.9973)\n",
      "2등: 006.jpg (유사도: 0.9973)\n",
      "3등: 002.jpeg (유사도: 0.9971)\n",
      "4등: 008.jpg (유사도: 0.9960)\n",
      "5등: 007.jpg (유사도: 0.9956)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. 모델 준비 (Places365용 ResNet18 구조)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 모델의 뼈대를 가져옵니다.\n",
    "model_places = models.resnet18(num_classes=365)\n",
    "# 마지막 분류 층(FC layer)을 제거하고 특징 추출기(Feature Extractor)로만 사용합니다.\n",
    "model_places = nn.Sequential(*(list(model_places.children())[:-1])).to(device)\n",
    "model_places.eval()\n",
    "\n",
    "# 2. 이미지 특징 추출 함수 정의 (이 부분이 'get_places_embedding'의 정체입니다!)\n",
    "def get_places_embedding(img_path):\n",
    "    # Places365/ResNet 표준 전처리\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_t = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 이미지를 모델에 통과시켜 특징 벡터를 뽑아냅니다.\n",
    "        features = model_places(img_t).flatten(1)\n",
    "    \n",
    "    # 유사도 비교를 위해 길이를 1로 맞추는(정규화) 과정입니다.\n",
    "    return features / features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# 3. 경로 설정 및 유사도 검색 실행\n",
    "query_img_path = \"img_forigen/001.jpg\"\n",
    "target_dir = \"img_korea\"\n",
    "\n",
    "if os.path.exists(query_img_path) and os.path.isdir(target_dir):\n",
    "    # 기준 이미지 특징 추출\n",
    "    query_emb = get_places_embedding(query_img_path)\n",
    "    \n",
    "    results = []\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.webp')\n",
    "\n",
    "    # 폴더 내 모든 한국 이미지와 비교\n",
    "    for file_name in os.listdir(target_dir):\n",
    "        if file_name.lower().endswith(valid_extensions):\n",
    "            target_path = os.path.join(target_dir, file_name)\n",
    "            target_emb = get_places_embedding(target_path)\n",
    "            \n",
    "            # 코사인 유사도 계산\n",
    "            score = F.cosine_similarity(query_emb, target_emb).item()\n",
    "            results.append((file_name, score))\n",
    "\n",
    "    # 결과 정렬 (높은 점수 순)\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\n[Places365 검색 결과 - 기준: {query_img_path}]\")\n",
    "    for i, (name, s) in enumerate(results[:5]):\n",
    "        print(f\"{i+1}등: {name} (유사도: {s:.4f})\")\n",
    "else:\n",
    "    print(\"경로를 찾을 수 없습니다. img_forigen 폴더와 img_korea 폴더를 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5d553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# ResNet18 기반의 Places365 가중치를 불러옵니다.\n",
    "model_places = models.resnet18(num_classes=365)\n",
    "# 공식 가중치 파일 URL (최초 1회 수동 로드 필요할 수 있음)\n",
    "storage_url = 'https://download.pytorch.org/models/resnet18-5c106cde.pth' # 기본 resnet 가중치와 구조가 같음\n",
    "# 실제로는 장소 분류를 위해 학습된 별도의 가중치 파일(.pth)을 로드해야 정확합니다.\n",
    "# 테스트용으로는 기본 ResNet의 특징 추출 능력을 활용해도 무방합니다.\n",
    "model_places = torch.nn.Sequential(*(list(model_places.children())[:-1])).to(device) \n",
    "model_places.eval()\n",
    "\n",
    "def get_places_embedding(img_path):\n",
    "    # 전처리는 일반적인 ResNet 방식 사용\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_t = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model_places(img_t).flatten(1)\n",
    "    return features / features.norm(p=2, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b560729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt V2 모델로 'img_korea' 검색 중...\n",
      "\n",
      "[ConvNeXt V2 검색 결과 - 기준: img_forigen/001.jpg]\n",
      "----------------------------------------\n",
      "1등: 001.jpg (유사도: 0.3324)\n",
      "2등: 003.JPG (유사도: 0.2836)\n",
      "3등: 006.jpg (유사도: 0.2199)\n",
      "4등: 004.JPG (유사도: 0.1856)\n",
      "5등: 009.jpg (유사도: 0.1479)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# 1. 모델 설정 및 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 'convnextv2_tiny' 모델을 사용합니다. 성능을 높이려면 'base'나 'large'로 변경 가능하지만, \n",
    "# 로컬 테스트용으로는 'tiny'가 속도 면에서 가장 적절합니다.\n",
    "model_name = 'convnextv2_tiny.fcmae_ft_in22k_in1k'\n",
    "model_convnext = timm.create_model(model_name, pretrained=True, num_classes=0).to(device)\n",
    "model_convnext.eval()\n",
    "\n",
    "# 모델에 맞는 전처리(Transform) 자동 생성\n",
    "data_config = timm.data.resolve_model_data_config(model_convnext)\n",
    "transform = timm.data.create_transform(**data_config, is_training=False)\n",
    "\n",
    "def get_convnext_embedding(img_path):\n",
    "    \"\"\"ConvNeXt V2를 사용해 이미지 특징 추출\"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_t = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model_convnext(img_t)\n",
    "        \n",
    "    # 유사도 비교를 위해 정규화\n",
    "    return features / features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "# 2. 경로 설정 (질문자님 디렉토리 기준)\n",
    "query_img_path = \"img_forigen/001.jpg\"\n",
    "target_dir = \"img_korea\"\n",
    "\n",
    "if os.path.exists(query_img_path) and os.path.isdir(target_dir):\n",
    "    # 기준 이미지 특징 추출\n",
    "    query_emb = get_convnext_embedding(query_img_path)\n",
    "    \n",
    "    results = []\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.webp')\n",
    "\n",
    "    # 3. 폴더 순회 및 유사도 계산\n",
    "    print(f\"ConvNeXt V2 모델로 '{target_dir}' 검색 중...\")\n",
    "    \n",
    "    for file_name in os.listdir(target_dir):\n",
    "        if file_name.lower().endswith(valid_extensions):\n",
    "            target_path = os.path.join(target_dir, file_name)\n",
    "            target_emb = get_convnext_embedding(target_path)\n",
    "            \n",
    "            # 코사인 유사도 계산\n",
    "            score = F.cosine_similarity(query_emb, target_emb).item()\n",
    "            results.append((file_name, score))\n",
    "\n",
    "    # 결과 정렬\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\n[ConvNeXt V2 검색 결과 - 기준: {query_img_path}]\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (name, s) in enumerate(results[:5]):\n",
    "        print(f\"{i+1}등: {name} (유사도: {s:.4f})\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "else:\n",
    "    print(\"경로를 확인해 주세요. img_forigen/001.jpg 파일과 img_korea 폴더가 필요합니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
