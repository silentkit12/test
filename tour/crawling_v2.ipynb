{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# VisitKorea Web Crawler\n",
                "\n",
                "이 노트북은 `https://korean.visitkorea.or.kr` 사이트의 여행지 정보를 크롤링합니다.\n",
                "\n",
                "## 주요 기능\n",
                "1. 목록 페이지에서 상세 페이지 링크 추출\n",
                "2. 상세 페이지에서 제목, 메타 정보(설명, 키워드), 이미지, 주소 추출\n",
                "3. `time.sleep`을 이용한 랜덤 딜레이 적용 (1~5초)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import time\n",
                "import random\n",
                "from urllib.parse import urljoin"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 상세 페이지 크롤링 함수\n",
                "\n",
                "개별 여행지 상세 페이지에서 필요한 정보를 추출하는 함수입니다.\n",
                "- `topTitle`\n",
                "- `meta[name=\"description\"]` content\n",
                "- `meta[name=\"keywords\"]` content\n",
                "- `img src` (모든 이미지)\n",
                "- 주소 (`<li>주소...<span>`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def crawl_detail_page(url):\n",
                "    try:\n",
                "        headers = {\n",
                "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
                "        }\n",
                "        response = requests.get(url, headers=headers)\n",
                "        response.raise_for_status()\n",
                "        soup = BeautifulSoup(response.text, 'html.parser')\n",
                "        \n",
                "        # 1. topTitle 추출\n",
                "        # id=\"topTitle\" 또는 class=\"topTitle\" 등을 시도합니다.\n",
                "        top_title = \"N/A\"\n",
                "        if soup.find(id=\"topTitle\"):\n",
                "            top_title = soup.find(id=\"topTitle\").get_text(strip=True)\n",
                "        elif soup.find(class_=\"topTitle\"):\n",
                "            top_title = soup.find(class_=\"topTitle\").get_text(strip=True)\n",
                "            \n",
                "        # 2. meta name=\"description\" content 추출\n",
                "        meta_desc = \"N/A\"\n",
                "        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})\n",
                "        if meta_desc_tag and 'content' in meta_desc_tag.attrs:\n",
                "            meta_desc = meta_desc_tag['content']\n",
                "            \n",
                "        # 3. meta name=\"keywords\" content 추출\n",
                "        meta_keywords = \"N/A\"\n",
                "        meta_keywords_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
                "        if meta_keywords_tag and 'content' in meta_keywords_tag.attrs:\n",
                "            meta_keywords = meta_keywords_tag['content']\n",
                "            \n",
                "        # 4. img src=\"url\" 모든 이미지 추출\n",
                "        images = []\n",
                "        for img in soup.find_all('img'):\n",
                "            src = img.get('src')\n",
                "            if src:\n",
                "                # 상대 경로인 경우 절대 경로로 변환\n",
                "                full_src = urljoin(url, src)\n",
                "                images.append(full_src)\n",
                "                \n",
                "        # 5. 주소 추출 (<li> 중 \"주소\" 텍스트 포함하는 요소의 <span>)\n",
                "        address = \"N/A\"\n",
                "        for li in soup.find_all('li'):\n",
                "            # 텍스트에 '주소'가 포함되어 있는지 확인\n",
                "            if \"주소\" in li.get_text():\n",
                "                span = li.find('span')\n",
                "                if span:\n",
                "                    address = span.get_text(strip=True)\n",
                "                break\n",
                "                \n",
                "        return {\n",
                "            \"url\": url,\n",
                "            \"title\": top_title,\n",
                "            \"description\": meta_desc,\n",
                "            \"keywords\": meta_keywords,\n",
                "            \"address\": address,\n",
                "            \"images_count\": len(images),\n",
                "            \"images\": images[:5] # 편의상 첫 5개만 출력 리스트에 포함 (전체는 필요 시 images 사용)\n",
                "        }\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error crawling {url}: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 목록 페이지 및 실행 로직\n",
                "\n",
                "목록 페이지에서 상세 페이지 링크를 수집하고, 각 링크에 대해 크롤링을 수행합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Searching for links in: https://korean.visitkorea.or.kr/main/area_list.do?type=Place\n",
                        "Found 0 additional links from the list page.\n",
                        "Note: No links found automatically. The page might be loading data via JavaScript/AJAX.\n",
                        "Proceeding with the example URL provided.\n",
                        "Total URLs to crawl: 1\n",
                        "[1/1] Crawling: https://korean.visitkorea.or.kr/detail/ms_detail.do?cotid=01b32883-1770-499e-9e0d-576736a55fbe\n",
                        "  -> Title: \n",
                        "  -> Address: 우편번호 찾기\n",
                        "  -> Description len: 249\n",
                        "\n",
                        "All crawling finished.\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# 1. 목록 페이지 설정\n",
                "list_url = \"https://korean.visitkorea.or.kr/main/area_list.do?type=Place\"\n",
                "detail_base_pattern = \"ms_detail.do\"\n",
                "\n",
                "# 크롤링할 대상 URL 리스트\n",
                "target_urls = []\n",
                "\n",
                "# 예시 URL 추가 (리스트 페이지 크롤링 실패 시 실행 확인용)\n",
                "example_url = \"https://korean.visitkorea.or.kr/detail/ms_detail.do?cotid=01b32883-1770-499e-9e0d-576736a55fbe\"\n",
                "target_urls.append(example_url)\n",
                "\n",
                "print(f\"Searching for links in: {list_url}\")\n",
                "try:\n",
                "    headers = {\n",
                "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
                "    }\n",
                "    # 목록 페이지 요청\n",
                "    resp = requests.get(list_url, headers=headers)\n",
                "    resp.raise_for_status()\n",
                "    list_soup = BeautifulSoup(resp.text, 'html.parser')\n",
                "    \n",
                "    # 링크 추출 시도\n",
                "    found_links = 0\n",
                "    for a in list_soup.find_all('a', href=True):\n",
                "        href = a['href']\n",
                "        if detail_base_pattern in href:\n",
                "            full_url = urljoin(list_url, href)\n",
                "            if full_url not in target_urls:\n",
                "                target_urls.append(full_url)\n",
                "                found_links += 1\n",
                "                \n",
                "    print(f\"Found {found_links} additional links from the list page.\")\n",
                "    if found_links == 0:\n",
                "        print(\"Note: No links found automatically. The page might be loading data via JavaScript/AJAX.\")\n",
                "        print(\"Proceeding with the example URL provided.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"Error getting list page: {e}\")\n",
                "    print(\"Proceeding with the example URL provided.\")\n",
                "\n",
                "print(f\"Total URLs to crawl: {len(target_urls)}\")\n",
                "\n",
                "# 2. 크롤링 실행 Loop\n",
                "crawled_data = []\n",
                "\n",
                "for i, url in enumerate(target_urls):\n",
                "    print(f\"[{i+1}/{len(target_urls)}] Crawling: {url}\")\n",
                "    \n",
                "    data = crawl_detail_page(url)\n",
                "    \n",
                "    if data:\n",
                "        crawled_data.append(data)\n",
                "        print(f\"  -> Title: {data['title']}\")\n",
                "        print(f\"  -> Address: {data['address']}\")\n",
                "        print(f\"  -> Description len: {len(data['description'])}\")\n",
                "    \n",
                "    # 3. Time Sleep (1~5초 랜덤)\n",
                "    if i < len(target_urls) - 1:\n",
                "        sleep_time = random.uniform(1, 5)\n",
                "        print(f\"  -> Sleeping for {sleep_time:.2f} seconds...\")\n",
                "        time.sleep(sleep_time)\n",
                "\n",
                "print(\"\\nAll crawling finished.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
