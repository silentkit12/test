"""
í•œêµ­ê´€ê´‘ê³µì‚¬ êµ¬ì„êµ¬ì„ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬
- ë©”ì¸ í˜ì´ì§€ì—ì„œ ì¥ì†Œ ì¹´ë“œ ëª©ë¡ ìˆ˜ì§‘
- ê° ì¥ì†Œì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
"""

import re
import time
import json
import argparse
from typing import List, Dict, Optional
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import pandas as pd


class VisitKoreaCrawler:
    """í•œêµ­ê´€ê´‘ê³µì‚¬ êµ¬ì„êµ¬ì„ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless: bool = False):
        """
        Args:
            headless: ë¸Œë¼ìš°ì €ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í• ì§€ ì—¬ë¶€
        """
        self.base_url = "https://korean.visitkorea.or.kr"
        self.driver = self._init_driver(headless)
        
    def _init_driver(self, headless: bool = False):
        """
        Chrome WebDriver ì´ˆê¸°í™”
        
        Args:
            headless: í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì—¬ë¶€
            
        Returns:
            WebDriver ì¸ìŠ¤í„´ìŠ¤
        """
        options = webdriver.ChromeOptions()
        
        if headless:
            options.add_argument('--headless')
        
        # ê¸°ë³¸ ì˜µì…˜
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # ìœ„ì¹˜ ì •ë³´, ì•Œë¦¼ ë“± ê¶Œí•œ ì°¨ë‹¨
        prefs = {
            "profile.default_content_setting_values": {
                "geolocation": 2,  # ìœ„ì¹˜ ì •ë³´ ì°¨ë‹¨ (1: í—ˆìš©, 2: ì°¨ë‹¨)
                "notifications": 2,  # ì•Œë¦¼ ì°¨ë‹¨
            }
        }
        options.add_experimental_option("prefs", prefs)
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        driver = webdriver.Chrome(options=options)
        driver.implicitly_wait(10)
        
        return driver
    
    def extract_location_ids(self, url: str = None, max_clicks: int = 100):
        if url is None:
            url = f"{self.base_url}/main/area_list.do?type=Place"
            
        print(f"ğŸ“ í˜ì´ì§€ ì ‘ì†: {url}")
        self.driver.get(url)
        
        # 1. ì´ˆê¸° ë¡œë”© ëŒ€ê¸° (ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ë³´ì¼ ë•Œê¹Œì§€)
        wait = WebDriverWait(self.driver, 15)
        try:
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "ul.list_thum_type.type1 li")))
        except:
            print("âš ï¸ í˜ì´ì§€ ë¡œë”©ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.")
        
        # ìœ„ì¹˜ ì •ë³´ íŒì—… ìë™ ë‹«ê¸°
        self._close_popups()
        time.sleep(2)  # íŒì—… ë‹«ì€ í›„ ëŒ€ê¸°
        
        # í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ì»¨í…ì¸  ë¡œë“œ
        self.driver.execute_script("window.scrollTo(0, 500);")
        time.sleep(1)
        
        # ë”ë³´ê¸° ë²„íŠ¼ì„ ê³„ì† í´ë¦­í•˜ì—¬ ëª¨ë“  ì¹´ë“œ ë¡œë“œ
        click_count = 0
        while click_count < max_clicks:
            try:
                # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
                more_button = None
                
                # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                selectors = [
                    "//button[contains(text(), 'ë”ë³´ê¸°')]",
                    "//button[contains(@class, 'btn_more')]",
                    "//a[contains(text(), 'ë”ë³´ê¸°')]",
                    "//a[contains(@class, 'more')]"
                ]
                
                for selector in selectors:
                    buttons = self.driver.find_elements(By.XPATH, selector)
                    for btn in buttons:
                        if btn.is_displayed() and btn.is_enabled():
                            more_button = btn
                            break
                    if more_button:
                        break
                
                if not more_button:
                    print(f"  â„¹ï¸  ë”ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì´ {click_count}íšŒ í´ë¦­)")
                    break
                
                # JavaScriptë¡œ ë²„íŠ¼ í´ë¦­ (element click intercepted ë°©ì§€)
                try:
                    self.driver.execute_script("arguments[0].click();", more_button)
                    click_count += 1
                    print(f"  ğŸ”„ ë”ë³´ê¸° í´ë¦­ {click_count}íšŒ")
                    time.sleep(2)  # ë¡œë”© ëŒ€ê¸°
                except Exception as e:
                    print(f"  âš ï¸  í´ë¦­ ì‹¤íŒ¨, ì¬ì‹œë„: {str(e)[:30]}")
                    # ìŠ¤í¬ë¡¤ í›„ ì¬ì‹œë„
                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", more_button)
                    time.sleep(1)
                    self.driver.execute_script("arguments[0].click();", more_button)
                    click_count += 1
                    print(f"  ğŸ”„ ë”ë³´ê¸° í´ë¦­ {click_count}íšŒ (ì¬ì‹œë„ ì„±ê³µ)")
                    time.sleep(2)
                
            except Exception as e:
                print(f"  â„¹ï¸  ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì¢…ë£Œ: {str(e)[:50]}")
                break
        print("â³ ë°ì´í„° íŒŒì‹± ì¤€ë¹„ ì¤‘...")
        time.sleep(3)

        # í˜ì´ì§€ ì†ŒìŠ¤ íŒŒì‹±
        soup = BeautifulSoup(self.driver.page_source, 'html.parser')

        location_ids = []
        # 1. idê°€ contentListì¸ ul ì•ˆì˜ ëª¨ë“  lië¥¼ ì°¾ìŠµë‹ˆë‹¤.
        items = soup.select("#contentList > li")

        for item in items:
            try:
                # 2. li ì•ˆì— ìˆëŠ” a íƒœê·¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                anchor = item.find('a')
                if not anchor:
                    continue
                    
                href_value = anchor.get('href', '')
                
                # 3. ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ coid= ë’¤ì˜ ID ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
                # [0-9a-f-] íŒ¨í„´ì€ ìˆ«ì, ì†Œë¬¸ì, í•˜ì´í”ˆì´ ì„ì¸ UUIDë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
                match = re.search(r"coid=([0-9a-f-]{36})", href_value)
                
                if match:
                    location_id = match.group(1)
                    if location_id not in location_ids:
                        location_ids.append(location_id)
                        
                        # 4. ì¥ì†Œëª… ì¶”ì¶œ (strong íƒœê·¸)
                        name_tag = anchor.find('strong')
                        name = name_tag.get_text(strip=True) if name_tag else "ì´ë¦„ ì—†ìŒ"
                        
                        print(f"  âœ“ {name} (ID: {location_id})")
                else:
                    # í˜¹ì‹œ coidê°€ ì—†ê³  goDetailPageë§Œ ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ 2ì°¨ ì‹œë„
                    match_alt = re.search(r"goDetailPage\('([0-9a-f-]{36})'\)", href_value)
                    if match_alt:
                        location_id = match_alt.group(1)
                        if location_id not in location_ids:
                            location_ids.append(location_id)
                            print(f"  âœ“ {name} (ID: {location_id} - secondary match)")

            except Exception as e:
                print(f"  âš ï¸ ê°œë³„ ìš”ì†Œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

            return location_ids
    
    def _close_popups(self):
        """
        íŒì—… ìë™ ë‹«ê¸° (ìœ„ì¹˜ ì •ë³´ ë™ì˜ ë“±)
        """
        try:
            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ íŒì—… ë‹«ê¸° ë²„íŠ¼ ì„ íƒì
            close_selectors = [
                "//button[contains(text(), 'ë‹«ê¸°')]",
                "//button[contains(text(), 'ì·¨ì†Œ')]",
                "//button[contains(@class, 'close')]",
                "//a[contains(@class, 'close')]",
                "//button[@aria-label='ë‹«ê¸°']",
                "//button[@aria-label='Close']",
            ]
            
            for selector in close_selectors:
                try:
                    buttons = self.driver.find_elements(By.XPATH, selector)
                    for btn in buttons:
                        if btn.is_displayed():
                            btn.click()
                            print("  âœ“ íŒì—… ë‹«ê¸° ì™„ë£Œ")
                            time.sleep(0.5)
                except:
                    continue
        except Exception as e:
            # íŒì—…ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
            pass
    
    def crawl_detail_info(self, location_id: str) -> Dict:
        """
        íŠ¹ì • ì¥ì†Œì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        
        Args:
            location_id: ì¥ì†Œ ID (cotid)
            
        Returns:
            ìƒì„¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        detail_url = f"{self.base_url}/detail/ms_detail.do?cotid={location_id}"
        print(f"ğŸ“„ ìƒì„¸ í˜ì´ì§€ ì ‘ì†: {location_id}")
        
        try:
            self.driver.get(detail_url)
            time.sleep(2)
            
            # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ì¥ì†Œëª…
            title_tag = soup.find('h2') or soup.find('h1', class_=re.compile(r'tit'))
            title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
            
            # 'ìƒì„¸ì •ë³´' íƒ­ í´ë¦­ (JavaScriptë¡œ ì§ì ‘ ì‹¤í–‰)
            try:
                self.driver.execute_script("tabChange('detailGo');")
                time.sleep(2)  # íƒ­ ì „í™˜ ëŒ€ê¸°
                print("  âœ“ ìƒì„¸ì •ë³´ íƒ­ í´ë¦­ ì™„ë£Œ")
            except Exception as e:
                print(f"  âš ï¸  ìƒì„¸ì •ë³´ íƒ­ í´ë¦­ ì‹¤íŒ¨: {str(e)}")
            
            # ìƒì„¸ì •ë³´ íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            detail_info = self._parse_detail_section(soup)
            
            # ì‚¬ì§„ URL ì¶”ì¶œ
            photo_urls = self._extract_photo_urls(soup)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'id': location_id,
                'name': title,
                'url': detail_url,
                'photo_urls': photo_urls,
                **detail_info
            }
            
            print(f"  âœ“ {title} - {len(detail_info)}ê°œ í•­ëª©, {len(photo_urls)}ê°œ ì‚¬ì§„ ìˆ˜ì§‘")
            return result
            
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                'id': location_id,
                'name': 'ì˜¤ë¥˜',
                'url': detail_url,
                'error': str(e)
            }
    
    def _parse_detail_section(self, soup: BeautifulSoup) -> Dict:
        """
        ìƒì„¸ì •ë³´ ì„¹ì…˜ íŒŒì‹±
        
        Args:
            soup: BeautifulSoup ê°ì²´
            
        Returns:
            ìƒì„¸ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        detail_info = {}
        
        # <li> íƒœê·¸ë¡œ êµ¬ì„±ëœ ì •ë³´ í•­ëª© ì°¾ê¸°
        list_items = soup.find_all('li')
        
        for item in list_items:
            # í•­ëª©ëª… (Label)
            label_tag = item.find('strong')
            if not label_tag:
                continue
                
            label = label_tag.text.strip()
            
            # í•­ëª©ê°’ (Value) - span íƒœê·¸ì—ì„œ ì¶”ì¶œ
            value_tags = item.find_all('span', class_='pc') or item.find_all('span')
            
            if value_tags:
                # ì—¬ëŸ¬ spanì´ ìˆì„ ê²½ìš° í•©ì¹˜ê¸°
                values = [tag.text.strip() for tag in value_tags if tag.text.strip()]
                value = ' / '.join(values) if values else ''
            else:
                # spanì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ label ì œê±°
                value = item.text.replace(label, '').strip()
            
            if value:
                detail_info[label] = value
        
        return detail_info
    
    def _extract_photo_urls(self, soup: BeautifulSoup) -> List[str]:
        """
        ì‚¬ì§„ URL ì¶”ì¶œ
        
        Args:
            soup: BeautifulSoup ê°ì²´
            
        Returns:
            ì‚¬ì§„ URL ë¦¬ìŠ¤íŠ¸
        """
        photo_urls = []
        
        # ì‚¬ì§„ë³´ê¸° ì˜ì—­ (id="galleryGo")ì—ì„œ swiper ìŠ¬ë¼ì´ë“œ ì°¾ê¸°
        gallery_section = soup.find('div', id='galleryGo')
        if not gallery_section or not hasattr(gallery_section, 'find_all'):
            return photo_urls
        
        # swiper-slide ë‚´ì˜ ëª¨ë“  img íƒœê·¸ ì°¾ê¸°
        slides = gallery_section.find_all('div', class_='swiper-slide')
        
        for slide in slides:
            img_tag = slide.find('img')
            if img_tag:
                # src ë˜ëŠ” data-src ì†ì„±ì—ì„œ URL ì¶”ì¶œ
                img_url = img_tag.get('src') or img_tag.get('data-src')
                if img_url and img_url.startswith('http'):
                    # URLì—ì„œ &amp; ë¥¼ & ë¡œ ë³€í™˜
                    img_url = img_url.replace('&amp;', '&')
                    if img_url not in photo_urls:
                        photo_urls.append(img_url)
        
        return photo_urls
    
    def crawl_multiple(self, location_ids: List[str], delay: float = 1.0) -> List[Dict]:
        """
        ì—¬ëŸ¬ ì¥ì†Œì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        
        Args:
            location_ids: ì¥ì†Œ ID ë¦¬ìŠ¤íŠ¸
            delay: ê° ìš”ì²­ ì‚¬ì´ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            
        Returns:
            ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        total = len(location_ids)
        
        for idx, location_id in enumerate(location_ids, 1):
            print(f"\n[{idx}/{total}] ", end='')
            result = self.crawl_detail_info(location_id)
            results.append(result)
            
            if idx < total:
                time.sleep(delay)
        
        return results
    
    def save_to_csv(self, data: List[Dict], filename: str):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   ì´ {len(data)}ê°œ í•­ëª©")
    
    def save_to_json(self, data: List[Dict], filename: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   ì´ {len(data)}ê°œ í•­ëª©")
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("\nğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(
        description='í•œêµ­ê´€ê´‘ê³µì‚¬ êµ¬ì„êµ¬ì„ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ì‚¬ìš© ì˜ˆì‹œ:
  # 10ê°œ ì¥ì†Œ í¬ë¡¤ë§
  python visitkorea_crawler.py --count 10
  
  # 50ê°œ ì¥ì†Œ í¬ë¡¤ë§, ë”ë³´ê¸° 30íšŒ í´ë¦­
  python visitkorea_crawler.py --count 50 --max-clicks 30
  
  # ë°±ê·¸ë¼ìš´ë“œ ëª¨ë“œë¡œ 100ê°œ í¬ë¡¤ë§
  python visitkorea_crawler.py --count 100 --headless
        '''
    )
    
    parser.add_argument(
        '--count', '-c',
        type=int,
        default=10,
        help='í¬ë¡¤ë§í•  ì¥ì†Œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)'
    )
    
    parser.add_argument(
        '--max-clicks', '-m',
        type=int,
        default=50,
        help='ë”ë³´ê¸° ë²„íŠ¼ ìµœëŒ€ í´ë¦­ íšŸìˆ˜ (ê¸°ë³¸ê°’: 50)'
    )
    
    parser.add_argument(
        '--headless',
        action='store_true',
        help='í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰ (ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¹€)'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='visitkorea_data',
        help='ì¶œë ¥ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸, ê¸°ë³¸ê°’: visitkorea_data)'
    )
    
    args = parser.parse_args()
    
    # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
    crawler = VisitKoreaCrawler(headless=args.headless)
    
    try:
        # 1. ì¥ì†Œ ID ìˆ˜ì§‘ (ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­)
        print("=" * 60)
        print("1ë‹¨ê³„: ì¥ì†Œ ID ìˆ˜ì§‘")
        print("=" * 60)
        location_ids = crawler.extract_location_ids(max_clicks=args.max_clicks)
        
        # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ í¬ë¡¤ë§
        target_count = min(args.count, len(location_ids))
        target_ids = location_ids[:target_count]
        print(f"\nğŸ“Š ì´ {len(location_ids)}ê°œ ì¤‘ {target_count}ê°œ ì¥ì†Œë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.\n")
        
        # 2. ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        print("=" * 60)
        print("2ë‹¨ê³„: ìƒì„¸ ì •ë³´ í¬ë¡¤ë§")
        print("=" * 60)
        results = crawler.crawl_multiple(target_ids, delay=1.5)
        
        # 3. ê²°ê³¼ ì €ì¥
        print("\n" + "=" * 60)
        print("3ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
        print("=" * 60)
        crawler.save_to_json(results, f'{args.output}.json')
        crawler.save_to_csv(results, f'{args.output}.csv')
        
        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! {len(results)}ê°œ ì¥ì†Œ ì •ë³´ ì €ì¥ë¨")
        
        print("\n" + "=" * 60)
        print("ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        print("=" * 60)
        for item in results[:2]:
            print(f"\nğŸ“ {item.get('name', 'ì´ë¦„ ì—†ìŒ')}")
            for key, value in item.items():
                if key not in ['id', 'name', 'url']:
                    print(f"   {key}: {value}")
        
    finally:
        crawler.close()


if __name__ == "__main__":
    main()